<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>Zora Zorkiƒá</title>
<meta name="description" content="The personal website of Zora Zorkiƒá">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://hzorkic.github.io/css/bootstrap.min.css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic">
<link rel="stylesheet" href="https://hzorkic.github.io/css/font-awesome.min.css">
<link rel="stylesheet" href="https://hzorkic.github.io/css/owl.carousel.css">
<link rel="stylesheet" href="https://hzorkic.github.io/css/owl.theme.css">


  <link href="https://hzorkic.github.io/css/style.default.css" rel="stylesheet" id="theme-stylesheet">

 

  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  


<link href="https://hzorkic.github.io/css/custom.css" rel="stylesheet">
<link rel="shortcut icon" href="https://hzorkic.github.io/img/favicon.png">



 
<script src="//ajax.googleapis.com/ajax/libs/jquery/2.2.1/jquery.min.js" ></script> 

<script type="text/javascript" src="js/bigfoot.js"></script> 
<link rel="stylesheet" type="text/css" href="css/bigfoot.css"> 

<script type="text/javascript"> $.bigfoot(); </script> 
</head>
<body>
  <div id="all">
      <div class="container-fluid">
          <div class="row row-offcanvas row-offcanvas-left">
              <div id="sidebar" class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas">
  <div class="sidebar-content" >
    
    <h1 class="sidebar-heading">
      <div class="hidden-xs"><img style="max-width: 70%; margin-bottom: 15px;  height: auto; border: 2px black solid; border-radius: 190px; max-height: 300px" src="/img/prof.png" alt=""></div>
      
      <a href="https://hzorkic.github.io/">
        
        <img src="https://see.fontimg.com/api/renderfont4/GOZEP/eyJyIjoiZnMiLCJoIjo2NSwidyI6MTAwMCwiZnMiOjY1LCJmZ2MiOiIjMDAwMDAwIiwiYmdjIjoiI0ZGRkZGRiIsInQiOjF9/em9yYSB6b3JraWM/sizelademo-regular.png" alt="Prehistoric fonts" style="height: auto; max-height: 80%; max-width: 100%; margin-bottom: 20px;">
      </a>
    </h1>
    
    
      <p class="sidebar-p" style="margin-bottom: 2px; ">Data Scientist.</p>
    
      <p class="sidebar-p" style="margin-bottom: 2px; ">UX Researcher.</p>
    
    
    
      <p class="sidebar-p" style="margin-top: 20px; "\>üìç Somewhere between Austin and San Diego </p>
    
    
   
    <ul class="sidebar-menu" style="margin-top: 20px; ">
      
        <li><a href="https://hzorkic.github.io/about/">About</a></li>
      
        <li><a href="https://hzorkic.github.io/portfolio/">Portfolio</a></li>
      
        <li><a href="https://hzorkic.github.io/blog/">Blog</a></li>
      
        <li><a href="https://docs.google.com/document/d/1kj4knE2wdZdFEqjAO-J8EwKpLbXFVES-eprmV8g3lqs/edit?usp=sharing">CV</a></li>
      
      
      
    </ul>
    <p class="social">
  
  
  
  
  
  <a href="mailto:hayley.zorkic@gmail.com" data-animate-hover="pulse" class="email" title="E-mail">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://www.linkedin.com/in/hayley-zorkic" data-animate-hover="pulse" class="external" title="LinkedIn" rel="me">
    <i class="fa fa-linkedin"></i>
  </a>
  
  
  
  <a href="https://github.com/hzorkic" data-animate-hover="pulse" class="external" title="GitHub" rel="me">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
</p>

      <div class="copyright" style="font-size: 2px;">
        
      </div>
    
    
    

    
  </div>
</div>

              
<div class="col-xs-12 col-sm-8 col-md-9 content-column white-background">
  <div class="small-navbar visible-xs">
  <button type="button" data-toggle="offcanvas" class="btn btn-ghost">
    <i class="fa fa-bars"></i>
  </button>
  <h1 class="small-navbar-heading">
    <a href="https://hzorkic.github.io/">
      <img src="https://see.fontimg.com/api/renderfont4/GOZEP/eyJyIjoiZnMiLCJoIjo2NSwidyI6MTAwMCwiZnMiOjY1LCJmZ2MiOiIjMDAwMDAwIiwiYmdjIjoiI0ZGRkZGRiIsInQiOjF9/em9yYSB6b3JraWM/sizelademo-regular.png"
      style="max-height:30px;">
    </a>
  </h1>
</div>

  <div class="row">
    <div class="col-lg-8">
      <div class="content-column-content">
         <h1>Naive Bayes Classification</h1>
         <p>For this week&rsquo;s homework we are going explore one new classification technique:</p>
<ul>
<li>Naive Bayes</li>
</ul>
<p>In this post we are going to use a dataset of Melboburne housing to predict the housing type as one of three possible categories:</p>
<ul>
<li>&lsquo;h&rsquo; house</li>
<li>&lsquo;u&rsquo; duplex</li>
<li>&rsquo;t&rsquo; townhouse</li>
</ul>
<p>In addition to building our own Naive Bayes classifier, we are going to compare the performace of our classifier to the <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes">Gaussian Naive Bayes Classifier</a> available in the scikit-learn library.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># import libraries</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> time
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> calendar
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.naive_bayes <span style="color:#f92672">import</span> GaussianNB 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Starting off loading a training set and setting a variable for the target column, &#34;Type&#34;</span>
</span></span><span style="display:flex;"><span>df_melb <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;https://gist.githubusercontent.com/yanyanzheng96/81b236aecee57f6cf65e60afd865d2bb/raw/56ddb53aa90c26ab1bdbfd0b8d8229c8d08ce45a/melb_data_train.csv&#39;</span>)
</span></span><span style="display:flex;"><span>target_col <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Type&#39;</span>
</span></span></code></pre></div><h2 id="q1---fix-a-column-of-data-to-be-numeric">Q1 - Fix a column of data to be numeric</h2>
<p>If we inspect our dataframe, <code>df_melb</code> using the <code>dtypes</code> method, we see that the column &ldquo;Date&rdquo; is an object.  However, we think this column might contain useful information so we want to convert it to <a href="https://en.wikipedia.org/wiki/Unix_time">seconds since epoch</a>. Use only the exiting imported libraries to create a new column &ldquo;unixtime&rdquo;. Be careful, the date strings in the file might have some non-uniform formating that you have to fix first.  Print out the min and max epoch time to check your work.  Drop the original &ldquo;Date&rdquo; column.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># normalize date accepts the date string as shown in the df_melb &#39;Date&#39; column,</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># and returns a data in a standardized format</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">standardize_date</span>(d):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># fix formatting of date string</span>
</span></span><span style="display:flex;"><span>    date <span style="color:#f92672">=</span> d<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#34;/&#34;</span>, <span style="color:#e6db74">&#34; &#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># define date formats</span>
</span></span><span style="display:flex;"><span>    format_short <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%d</span><span style="color:#e6db74"> %m %y&#34;</span>
</span></span><span style="display:flex;"><span>    format_long <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">%d</span><span style="color:#e6db74"> %m %Y&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># change date to time in seconds based on one of two cases</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> len(date) <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">10</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">return</span> time<span style="color:#f92672">.</span>strptime(date, format_short)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> len(date) <span style="color:#f92672">==</span> <span style="color:#ae81ff">10</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">return</span> time<span style="color:#f92672">.</span>strptime(date, format_long)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df_melb[<span style="color:#e6db74">&#39;Date&#39;</span>] <span style="color:#f92672">=</span> df_melb[<span style="color:#e6db74">&#39;Date&#39;</span>]<span style="color:#f92672">.</span>apply( standardize_date )
</span></span><span style="display:flex;"><span>df_melb[<span style="color:#e6db74">&#39;unixtime&#39;</span>] <span style="color:#f92672">=</span> df_melb[<span style="color:#e6db74">&#39;Date&#39;</span>]<span style="color:#f92672">.</span>apply( <span style="color:#66d9ef">lambda</span> x: int(time<span style="color:#f92672">.</span>mktime(x)))
</span></span><span style="display:flex;"><span>df_melb <span style="color:#f92672">=</span> df_melb<span style="color:#f92672">.</span>drop(columns<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Date&#34;</span>)
</span></span></code></pre></div><h2 id="q2-calculating-the-prior-probabilities">Q2 Calculating the prior probabilities</h2>
<p>Calculate the prior probabilities for each possible &ldquo;Type&rdquo; in <code>df_melb</code> and populate a dictionary, <code>dict_priors</code>, where the key is the possible &ldquo;Type&rdquo; values and the value is the prior probabilities. Show the dictionary. Do not hardcode the possible values of &ldquo;Type&rdquo;.  Don&rsquo;t forget about <a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html">value counts</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dict_priors <span style="color:#f92672">=</span> (df_melb[<span style="color:#e6db74">&#39;Type&#39;</span>]<span style="color:#f92672">.</span>value_counts()<span style="color:#f92672">/</span>df_melb[<span style="color:#e6db74">&#39;Type&#39;</span>]<span style="color:#f92672">.</span>size)<span style="color:#f92672">.</span>to_dict()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># show the priors</span>
</span></span><span style="display:flex;"><span>dict_priors
</span></span></code></pre></div><pre><code>{'h': 0.452, 't': 0.13, 'u': 0.418}
</code></pre>
<h2 id="q3-create-a-model-for-the-distribution-of-all-of-the-numeric-attributes">Q3 Create a model for the distribution of all of the numeric attributes</h2>
<p>For each class, and for each attribute calculate the sample mean and sample standard deviation.  You should store the model in a nested dictionary, <code>dict_nb_model</code>, such that <code>dict_nb_model['h']['Rooms']</code> is a tuple containing the mean and standard deviation for the target Type &lsquo;h&rsquo; and the attribute &lsquo;Rooms&rsquo;.  Show the model using the <code>display</code> function. You should ignore entries that are <code>NaN</code> in the mean and <a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.std.html">standard deviation</a> calculation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dict_nb_model <span style="color:#f92672">=</span> dict()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> target <span style="color:#f92672">in</span> dict_priors<span style="color:#f92672">.</span>keys():
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># create temporary df for each housing type and add dict to dict_nb</span>
</span></span><span style="display:flex;"><span>    df_type <span style="color:#f92672">=</span> df_melb[df_melb[<span style="color:#e6db74">&#39;Type&#39;</span>] <span style="color:#f92672">==</span> target]<span style="color:#f92672">.</span>drop(columns<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Type&#39;</span>)
</span></span><span style="display:flex;"><span>    dict_nb_model[target] <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># find mean, std, save as tuple in dict_nb</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> df_type<span style="color:#f92672">.</span>columns:
</span></span><span style="display:flex;"><span>      mean <span style="color:#f92672">=</span> df_type[col]<span style="color:#f92672">.</span>mean(skipna <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      std <span style="color:#f92672">=</span> df_type[col]<span style="color:#f92672">.</span>std(skipna <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>      dict_nb_model[target][col] <span style="color:#f92672">=</span> (mean, std)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>display(dict_nb_model)
</span></span></code></pre></div><pre><code>{'h': {'Bathroom': (1.5619469026548674, 0.6720871086493075),
  'BuildingArea': (156.24339622641511, 54.62662837301434),
  'Car': (1.7777777777777777, 0.932759177140425),
  'Distance': (12.086725663716809, 7.397501132737295),
  'Landsize': (932.9646017699115, 3830.7934157687164),
  'Postcode': (3103.8982300884954, 98.35750345419703),
  'Price': (1189022.3451327435, 586296.5794417895),
  'Rooms': (3.269911504424779, 0.7258264201127756),
  'YearBuilt': (1954.900826446281, 32.461876347154686),
  'unixtime': (1485717578.761062, 13838562.050601462)},
 't': {'Bathroom': (1.8461538461538463, 0.565430401076506),
  'BuildingArea': (138.66666666666666, 53.498637054290135),
  'Car': (1.6923076923076923, 0.5280588545286915),
  'Distance': (10.766153846153845, 4.870455475462387),
  'Landsize': (268.18461538461537, 276.57700624711265),
  'Postcode': (3121.6153846153848, 100.01588816090862),
  'Price': (1000169.2307692308, 421822.5363389935),
  'Rooms': (2.9076923076923076, 0.6052653582075831),
  'YearBuilt': (1997.0227272727273, 16.99177453038181),
  'unixtime': (1486525292.3076923, 12640127.60987191)},
 'u': {'Bathroom': (1.1818181818181819, 0.42228151548662185),
  'BuildingArea': (83.85585585585585, 45.959438015166604),
  'Car': (1.1483253588516746, 0.47231993860296956),
  'Distance': (8.760287081339715, 5.609778714430755),
  'Landsize': (436.23444976076553, 1394.3403794653254),
  'Postcode': (3120.4545454545455, 87.18475679946482),
  'Price': (634207.1770334928, 217947.32866736987),
  'Rooms': (2.0430622009569377, 0.5908453859944267),
  'YearBuilt': (1976.451388888889, 24.557291330642666),
  'unixtime': (1484176719.617225, 13494566.111289721)}}
</code></pre>
<h2 id="q4-write-a-function-that-calculates-the-probability-of-a-gaussian">Q4 Write a function that calculates the probability of a Gaussian</h2>
<p>Given the mean ($\mu$), standard deviation ($\sigma$), and a observed point, <code>x</code>, return the probability.<br>
Use the formula $p(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$ (<a href="https://en.wikipedia.org/wiki/Normal_distribution">wiki</a>).  You should use <a href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">numpy&rsquo;s exp</a> function in your solution.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_p</span>( mu, sigma, x):
</span></span><span style="display:flex;"><span>    exponent <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> ((x<span style="color:#f92672">-</span>mu)<span style="color:#f92672">/</span>sigma)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>    multiplier <span style="color:#f92672">=</span> (sigma <span style="color:#f92672">*</span> (<span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>pi)<span style="color:#f92672">**</span><span style="color:#ae81ff">.5</span>)<span style="color:#f92672">**-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    prob <span style="color:#f92672">=</span> multiplier <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>exp(exponent)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> prob
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Test it</span>
</span></span><span style="display:flex;"><span>p <span style="color:#f92672">=</span> get_p( <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>p
</span></span></code></pre></div><pre><code>0.19333405840142462
</code></pre>
<h2 id="q5-write-the-naive-bayes-classifier-function">Q5 Write the Naive Bayes classifier function</h2>
<p>The Naive Bayes classifier function, <code>nb_class</code>,</p>
<p>should take as a parameter the prior probability dictionary. <code>dict_priors</code>,</p>
<p>the dictionary containing all of the gaussian distribution information for each attribue, <code>dict_nb_model</code>,</p>
<p>and a single observation row (a series generated from iterrows) of the test dataframe.</p>
<p>It should return a single target classification.</p>
<p>For this problem, all of our attributes are numeric and modeled as Gaussians, so we don&rsquo;t worry about categorical data. Make sure to skip attributes that do not have a value in the observation.  Do not hardcode the possible classification types.</p>
<pre><code>&quot;def nb_class( dict_priors, dict_nb_model, observation):\n    best_prob = 0.0\n    prob_type = 'none'  # this default SHOULDN'T be found in output but is!\n    for target in dict_priors.keys():   # potential types for classification\n      prob = dict_priors[target]\n      for key, value in dict_nb_model[target].items(): # get all the tuples from dict:\n        prob *= get_p( value[0], value[1], observation[key])  # multiply by conditional prob for each attribute\n      if prob &gt;= best_prob:\n        best_prob = prob\n        prob_type = target\n    return prob_type&quot;
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">nb_class</span>( dict_priors, dict_nb_model, observation):
</span></span><span style="display:flex;"><span>  probs <span style="color:#f92672">=</span> dict()
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> letter <span style="color:#f92672">in</span> dict_priors:
</span></span><span style="display:flex;"><span>      probs[letter] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># for each possiblbe letter in dict_priors</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> letter <span style="color:#f92672">in</span> dict_priors:
</span></span><span style="display:flex;"><span>    prob <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># for each varialbe in the row</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> attribute <span style="color:#f92672">in</span> dict_nb_model[letter]:
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># if row is na or in the dict keys</span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">if</span> (pd<span style="color:#f92672">.</span>isna(observation[attribute]) <span style="color:#f92672">is</span> <span style="color:#66d9ef">True</span> ) <span style="color:#f92672">or</span> (observation[attribute] <span style="color:#f92672">in</span> [list(dict_priors<span style="color:#f92672">.</span>keys())]):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        mean, std <span style="color:#f92672">=</span> dict_nb_model[letter][attribute]
</span></span><span style="display:flex;"><span>        prob <span style="color:#f92672">*=</span> get_p(mean, std, float(observation[attribute]))
</span></span><span style="display:flex;"><span>    prob <span style="color:#f92672">*=</span> dict_priors[letter]
</span></span><span style="display:flex;"><span>    probs[letter] <span style="color:#f92672">=</span> prob
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get the class</span>
</span></span><span style="display:flex;"><span>    classy <span style="color:#f92672">=</span> max(probs, key<span style="color:#f92672">=</span>probs<span style="color:#f92672">.</span>get)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> classy
</span></span></code></pre></div><h2 id="q6a-calculate-the-accuracy-using-naive-bayes-classifier-function-on-the-test-set">Q6a. Calculate the accuracy using Naive Bayes classifier function on the test set</h2>
<p>Load the test set from file, convert date to unix time and drop the date column, classify each row using your <code>nb_class</code>, and then show the accuracy on the test set.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df_test <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#39;https://gist.githubusercontent.com/yanyanzheng96/c3d53303cebbd986b166591d19254bac/raw/94eb3b2d500d5f7bbc0441a8419cd855349d5d8e/melb_data_test.csv&#39;</span>)
</span></span><span style="display:flex;"><span>df_test[<span style="color:#e6db74">&#39;Date&#39;</span>] <span style="color:#f92672">=</span> df_test[<span style="color:#e6db74">&#39;Date&#39;</span>]<span style="color:#f92672">.</span>apply( standardize_date )
</span></span><span style="display:flex;"><span>df_test[<span style="color:#e6db74">&#39;unixtime&#39;</span>] <span style="color:#f92672">=</span> df_test[<span style="color:#e6db74">&#39;Date&#39;</span>]<span style="color:#f92672">.</span>apply( <span style="color:#66d9ef">lambda</span> x: int(time<span style="color:#f92672">.</span>mktime(x)))
</span></span><span style="display:flex;"><span>df_test <span style="color:#f92672">=</span> df_test<span style="color:#f92672">.</span>drop(columns<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Date&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>predictions <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> (index,row) <span style="color:#f92672">in</span> df_test<span style="color:#f92672">.</span>iterrows():
</span></span><span style="display:flex;"><span>    predictions<span style="color:#f92672">.</span>append(nb_class(dict_priors, dict_nb_model, row))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>acc <span style="color:#f92672">=</span> sum(<span style="color:#ae81ff">1</span> <span style="color:#66d9ef">for</span> x,y <span style="color:#f92672">in</span> zip(list(df_test[<span style="color:#e6db74">&#39;Type&#39;</span>]),predictions) <span style="color:#66d9ef">if</span> x <span style="color:#f92672">==</span> y) <span style="color:#f92672">/</span> float(len(list(df_test[<span style="color:#e6db74">&#39;Type&#39;</span>])))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Accuracy with custom is </span><span style="color:#e6db74">{:.0f}</span><span style="color:#e6db74">%&#39;</span><span style="color:#f92672">.</span>format(acc<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>))
</span></span><span style="display:flex;"><span><span style="color:#75715e"># df_test[&#39;Type&#39;].compare(pd.Series(predictions))</span>
</span></span></code></pre></div><pre><code>Accuracy with custom is 57%
</code></pre>
<h2 id="q6b-use-scikit-learn-to-do-the-same-thing">Q6b. Use scikit-learn to do the same thing!</h2>
<p>Now we understand the inner workings of the Naive Bayes algorithm, let&rsquo;s compare our results to <a href="https://scikit-learn.org/stable/modules/naive_bayes.html">scikit-learn&rsquo;s Naive Bayes</a> implementation. Use the <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes">GaussianNB</a> to train using the <code>df_melb</code>dataframe and test using the <code>df_test</code> dataframe. Remember to split <code>df_melb</code> into a <code>df_X</code> with the numerical attributes, and a <code>s_y</code> with the target column. On the <code>df_melb</code> frame you will have to fill the empty attributes via imputation since the scikit-learn library can not handle missing values.  Use the same method you used in the last homework (filling the training data with the mean of the non-nan values).</p>
<p>Answer the following in a markdown cell: do you think imputation hurt or helped the classifier?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Imputation - apply on the test data</span>
</span></span><span style="display:flex;"><span>dict_imputation <span style="color:#f92672">=</span> dict()
</span></span><span style="display:flex;"><span>target_col <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Type&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> col <span style="color:#f92672">in</span> df_melb<span style="color:#f92672">.</span>columns:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> col <span style="color:#f92672">!=</span> target_col:
</span></span><span style="display:flex;"><span>      mean <span style="color:#f92672">=</span> df_melb[col]<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>      dict_imputation[col] <span style="color:#f92672">=</span> mean
</span></span><span style="display:flex;"><span>      df_melb[col] <span style="color:#f92672">=</span> df_melb[col]<span style="color:#f92672">.</span>fillna(mean)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Seperate the attributes from the target_col</span>
</span></span><span style="display:flex;"><span>s_y <span style="color:#f92672">=</span> df_melb[<span style="color:#e6db74">&#39;Type&#39;</span>]
</span></span><span style="display:flex;"><span>df_X <span style="color:#f92672">=</span> df_melb<span style="color:#f92672">.</span>drop(columns<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Type&#39;</span>)
</span></span><span style="display:flex;"><span>df_test_nan_free <span style="color:#f92672">=</span> df_test<span style="color:#f92672">.</span>copy()<span style="color:#f92672">.</span>dropna()
</span></span><span style="display:flex;"><span>s_y_test <span style="color:#f92672">=</span> df_test_nan_free[<span style="color:#e6db74">&#39;Type&#39;</span>]
</span></span><span style="display:flex;"><span>df_X_test <span style="color:#f92672">=</span> df_test_nan_free<span style="color:#f92672">.</span>drop(columns<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Type&#39;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># run Native Bayes Classifier</span>
</span></span><span style="display:flex;"><span>gnb <span style="color:#f92672">=</span> GaussianNB()
</span></span><span style="display:flex;"><span>y_pred <span style="color:#f92672">=</span> gnb<span style="color:#f92672">.</span>fit(df_X, s_y)<span style="color:#f92672">.</span>predict(df_X_test)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>acc <span style="color:#f92672">=</span> sum(<span style="color:#ae81ff">1</span> <span style="color:#66d9ef">for</span> x,y <span style="color:#f92672">in</span> zip(list(s_y_test),y_pred) <span style="color:#66d9ef">if</span> x <span style="color:#f92672">==</span> y) <span style="color:#f92672">/</span> float(len(list(s_y_test)))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Accuracy is </span><span style="color:#e6db74">{:.0f}</span><span style="color:#e6db74">%&#39;</span><span style="color:#f92672">.</span>format(acc<span style="color:#f92672">*</span><span style="color:#ae81ff">100</span>))
</span></span></code></pre></div><pre><code>Accuracy is 34%
</code></pre>
<h2 id="answer-to-do-you-think-imputation-hurt-or-helped-the-classifier">ANSWER TO &ldquo;do you think imputation hurt or helped the classifier?&rdquo;</h2>
<p>Because our accuracy decreased, imputation actually hurt our classifier.</p>

         
      </div>
    </div>
  </div>
</div>

          </div>
      </div>
  </div>
  <script src="https://hzorkic.github.io/js/jquery.min.js"></script>
<script src="https://hzorkic.github.io/js/bootstrap.min.js"></script>
<script src="https://hzorkic.github.io/js/jquery.cookie.js"> </script>
<script src="https://hzorkic.github.io/js/ekko-lightbox.js"></script>
<script src="https://hzorkic.github.io/js/jquery.scrollTo.min.js"></script>
<script src="https://hzorkic.github.io/js/masonry.pkgd.min.js"></script>
<script src="https://hzorkic.github.io/js/imagesloaded.pkgd.min.js"></script>
<script src="https://hzorkic.github.io/js/owl.carousel.min.js"></script>
<script src="https://hzorkic.github.io/js/front.js"></script>



</body>
</html>
