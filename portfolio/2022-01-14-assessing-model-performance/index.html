<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>HAYLEY ZORKIC</title>
<meta name="description" content="The Personal Website of Hayley Zorkic">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://hzorkic.github.io/css/bootstrap.min.css">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Roboto:400,300,700,400italic">
<link rel="stylesheet" href="https://hzorkic.github.io/css/font-awesome.min.css">
<link rel="stylesheet" href="https://hzorkic.github.io/css/owl.carousel.css">
<link rel="stylesheet" href="https://hzorkic.github.io/css/owl.theme.css">


  <link href="https://hzorkic.github.io/css/style.default.css" rel="stylesheet" id="theme-stylesheet">

 

  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  


<link href="https://hzorkic.github.io/css/custom.css" rel="stylesheet">
<link rel="shortcut icon" href="https://hzorkic.github.io/img/aobut.png">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-EGR7PE3Y44', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>
<body>
  <div id="all">
      <div class="container-fluid">
          <div class="row row-offcanvas row-offcanvas-left">
              <div id="sidebar" class="col-xs-6 col-sm-4 col-md-3 sidebar-offcanvas">
  <div class="sidebar-content">
    
    <img style="border: solid 2px black; border-radius: 50%; max-width: 90%; height: auto;" src="/img/1_about.jpg" alt="">
    <h1></h1>
    
    <h1 class="sidebar-heading"><a href="https://hzorkic.github.io/">HAYLEY ZORKIC</a></h1>
    
      <p class="sidebar-p">Data Scientist. Researcher.</p>
    
    
    <ul class="sidebar-menu">
      
        <li><a href="https://hzorkic.github.io/about/">About</a></li>
      
        <li><a href="https://hzorkic.github.io/portfolio/">Technical</a></li>
      
        <li><a href="https://hzorkic.github.io/creative/">Creative</a></li>
      
        <li><a href="https://hzorkic.github.io/img/RESUME.pdf">CV</a></li>
      
    </ul>
    
    <p class="social">
  
  
  
  
  
  <a href="mailto:hayley.zorkic@gmail.com" data-animate-hover="pulse" class="email">
    <i class="fa fa-envelope"></i>
  </a>
  
  
  <a href="https://www.linkedin.com/in/hayley-zorkic" data-animate-hover="pulse" class="external">
    <i class="fa fa-linkedin"></i>
  </a>
  
  
  
  <a href="https://github.com/hzorkic" data-animate-hover="pulse" class="external">
    <i class="fa fa-github"></i>
  </a>
  
  
  
  
</p>

    <p><sub><i> Note: Some posts are under construction and subject to change. </i></sub></p>
  </div>
</div>

              
<div class="col-xs-12 col-sm-8 col-md-9 content-column white-background">
  <div class="small-navbar visible-xs ">
  
  <button type="button" data-toggle="offcanvas" class="btn btn-ghost pull-left">
    <i class="fa fa-bars"></i>
  </button>

  <h1 class="small-navbar-heading"><a href="https://hzorkic.github.io/">HAYLEY ZORKIC</a></h1>
  
</div>


  <div class="row">
    <div class="col-lg-8">
      <div class="content-column-content">
         <h1>Assessing Model Performance </h1>
         <h5>
            Posted on Jan 14, 2022 - 
            <i class="fa fa-clock-o"></i> 
           4 min read 
        </h5>
         <p>We need to ensure our models perform well and have a positive impact when deployed for business use. There are several ways to quantify the performance of Machine Learning models. There are a few performance metrics that are more exclusive to Machine Learning, but there are metrics that overlap with statistics. If you&rsquo;ve taken a statistics class before, you may have heard of a few performance metrics before like:</p>
<ul>
<li>True Positive Rate</li>
<li>False Positive Rate</li>
<li>Precision</li>
<li>Accuracy</li>
</ul>
<p>What performance metric you use depends on what mind of model you&rsquo;ve created. Typically, we are performing classification or regression, but we can use other models as well. Below is a picture showing some common metrics used in each kind of model.</p>
<p><img src="/img/portfolio/metrics.png" alt="Metrics chart"></p>
<h2 id="classification-metrics">Classification Metrics</h2>
<p>When evaluating the performance of a classification model, it is essential to understand the distinction between the true outcome and predicted outcome.</p>
<p><strong>True Positive (TP):</strong> A sample is predicted to be positive (ŷ=1, e.g. the person is predicted to develop the disease) and its label is actually positive (y=1, e.g. the person will actually develop the disease).</p>
<p><strong>True Negative (TN):</strong> A sample is predicted to be negative (ŷ=0, e.g. the person is predicted to not develop the disease) and its label is actually negative (y=0, e.g. the person will actually not develop the disease).</p>
<p><strong>False Positive (FP):</strong> A sample is predicted to be positive (ŷ=1, e.g. the person is predicted to develop the disease) and its label is actually negative (y=0, e.g. the person will actually not develop the disease). In this case, the sample is “falsely” predicted as positive.</p>
<p><strong>False Negative (FN):</strong> A sample is predicted to be negative (ŷ=0, e.g. the person is predicted to not develop the disease) and its label is actually positive (y=1, e.g. the person will actually develop the disease). In this case, the sample is “falsely” predicted as negative.</p>
<p>This can be a little bit confusing, but the way I think about it is whether or not the classification is &ldquo;truly&rdquo; or &ldquo;falsely&rdquo; positive or negative.</p>
<p>We can combine all of these outcomes into the Confusion Matrix.</p>
<p><img src="/img/portfolio/small_confusion_matrix.png" alt="small confusion matrix"></p>
<p>When we create performance metrics from these outcomes, our confusion matrix gets a lot more confusing&hellip; so let&rsquo;s break it down.</p>
<p><img src="/img/portfolio/conf_matrix.png" alt="expanded confusion matrix"></p>
<p><strong>Accuracy (ACC):</strong> The fraction of predictions our model got right out of all the predictions including truly negative negatives and truly positive positives. Accuracy ranges between 0 and 1. If our model is perfect, meaning it has no FP or FN, the accuracy is 1. If our model is perfectly bad, it has no TP or TN.</p>
<pre tabindex="0"><code class="language-{math}" data-lang="{math}">Accuracy = (TP + TN)/( TP + TN + FP + FN)
</code></pre><p>Accuracy is not the best metric however, as it is susceptible to imbalanced data. For example, if we build a model that predicted YES 100% of the time, and our sample consisted of 95 YES and 5 NO, Technically, our model has 95% accuracy, but the model isn&rsquo;t really predicting anything at all.</p>
<p><strong>Precision (PPV):</strong> The proportion of positive predictions that were actually correct.</p>
<pre tabindex="0"><code class="language-{math}" data-lang="{math}">Precision = (TP)/(TP + FP)
</code></pre><p><strong>Recall (Sensitivity):</strong> Recall tells use how many of the actually positive cases were predicted correctly. You would use recall when reporting false negatives is more important than false positives. Doctors, for example, would probably signal a false positive for a patient to have disease X when they don&rsquo;t</p>
<pre tabindex="0"><code class="language-{math}" data-lang="{math}">Precision = (TP)/(TP + FN)
</code></pre><p><strong>F1 Score:</strong> The F1 score combines Precision and Recall. The value is maximized when Precision is equal to Recall, thus the score is penalized when there are extreme imbalances in Precision and Recall scores. Typically, F1 scores are used when:</p>
<ul>
<li>FP and FN are equally important</li>
<li>TN are high</li>
</ul>
<pre tabindex="0"><code class="language-{math}" data-lang="{math}">F1 Score = TP / ( TP + 0.5(FP + FN) )
</code></pre><p><strong>AUC-ROC:</strong> AUC-ROC may seem like alphabet soup, however is is a very important and well-loved way to visualize model performance at a variety of thresholds. The ROC-AUC plots the True Positive Rate against the False Positive Rate. In order to better understand what ROC-AUC, let&rsquo;s break it down.</p>
<p>Area Under the Curve (AUC) is the measure of the ability for the classier to distinguish between classes. The greater the AUC, the better the performance ht model is at different therhold points between positive and negative classes. So, when AUC is 1, the calssifier can distinguish all Positives and Negatives perfectly and if the value is 0, the classifier does not distinguish between classes.</p>
<p><img src="/img/under_construction.png" alt=""></p>

         
      </div>
    </div>
  </div>
</div>

          </div>
      </div>
  </div>
  <script src="https://hzorkic.github.io/js/jquery.min.js"></script>
<script src="https://hzorkic.github.io/js/bootstrap.min.js"></script>
<script src="https://hzorkic.github.io/js/jquery.cookie.js"> </script>
<script src="https://hzorkic.github.io/js/ekko-lightbox.js"></script>
<script src="https://hzorkic.github.io/js/jquery.scrollTo.min.js"></script>
<script src="https://hzorkic.github.io/js/masonry.pkgd.min.js"></script>
<script src="https://hzorkic.github.io/js/imagesloaded.pkgd.min.js"></script>
<script src="https://hzorkic.github.io/js/owl.carousel.min.js"></script>
<script src="https://hzorkic.github.io/js/front.js"></script>



</body>
</html>
